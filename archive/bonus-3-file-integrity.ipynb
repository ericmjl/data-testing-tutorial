{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# File Integrity\n",
    "\n",
    "With file integrity, the basic question we are answering is: \"Has the file changed since the last time you used it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Hash (Browns)\n",
    "\n",
    "File integrity can be checked by checking the \"hash\" of a file. \n",
    "\n",
    "The layman definition of a hash: A fixed-length, scrambled string that uniquely identifies \"a thing\".\n",
    "\n",
    "The layman definition of a hashing function: A function that transforms \"a thing\" into a hash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `hashlib`\n",
    "\n",
    "`hashlib` is part of the Python standard library, and it provides a library of hashing functions for hashing objects, strings, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import sha256, md5\n",
    "\n",
    "m = sha256()\n",
    "m.update('hello'.encode('utf-8'))\n",
    "m.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties of hashes & `hashlib` functions\n",
    "\n",
    "The first property of hashes is that of the same \"thing\" should yield the same hash value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = sha256()\n",
    "m2.update('hello'.encode('utf-8'))\n",
    "m2.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar-looking but different strings will yield different hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3c48591d8d098a4538f5e013dfcf406e948eac4d3277b10bf614e295d6068179'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = sha256()\n",
    "m3.update('h√©llo'.encode('utf-8'))\n",
    "m3.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using a different hashing algorithm will yield a different hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5d41402abc4b2a76b9719d911017c592'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = md5()\n",
    "n.update('hello'.encode('utf-8'))\n",
    "n.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hashing functions don't work on all objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers cannot be hashed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    o = sha256()\n",
    "    o.update(3)\n",
    "except TypeError:\n",
    "    print('Numbers cannot be hashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings cannot be hashed without encoding.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    o = sha256()\n",
    "    o.update('Hello world!')\n",
    "except TypeError:\n",
    "    print('Strings cannot be hashed without encoding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0535e4be2b79ffd93291305436bf889314e4a3faec05ecffcbb7df31ad9e51a\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    o = sha256()\n",
    "    o.update('Hello world!'.encode('utf-8'))\n",
    "    print(o.hexdigest())\n",
    "except TypeError:\n",
    "    print('Strings must be encoded first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Checking for changes in data file\n",
    "\n",
    "Multiple approaches possible:\n",
    "\n",
    "- Check every cell against a \"master\" copy, assuming you have one. (**inefficient, but good for pinpointing tampered cells**)\n",
    "- Check every row against a hash of that row. (**somewhat inefficient, but good for practice, and good for pinpointing tampered rows**)\n",
    "- Check hash of a file. (**most efficient way**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good pragmatic balance is to check every row against a hash of that row; storing the hash of the row may help us pinpoint which row may have been tampered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise Part 1\n",
    "\n",
    "- Write a convenience function that hashes strings and returns the digest, and add it to `datafuncs.py`. It should wrap the SHA256 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise Part 2\n",
    "\n",
    "Inside `datafuncs.py`, write a utility function with the signature `hash_data(handle)`, that does the following:\n",
    "\n",
    "- Use `pandas` to open the data file as specified by the `handle` as the variable `df`.\n",
    "- Create a new DataFrame called `hashes`.\n",
    "- Create a new column in `hashes` called `concat`, which is each column of data from `df` converted to strings and concatenated into a contiguous string.\n",
    "- Create a new column in `hashes` called `hash`, which is the computed the hash of each row of the contiguous strings.\n",
    "- Delete the `concat` column from `hashes`.\n",
    "- Save the hashes to disk as the file `hashes.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Part 3\n",
    "\n",
    "- Now, write a function `test_divvy_corrupt()`, that lets us compare the two CSV files and automatically finds out which row has corrupted data. You will need to import the functions previously written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hash of a file\n",
    "\n",
    "It is possible to check the hash of a file. Let's add an existing implementation found online to our toolkit, `datafuncs.py`.\n",
    "\n",
    "(All credit to StackOverflow community: http://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def hash_file(fname):\n",
    "    filehash = sha256()\n",
    "    with open(fname, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            filehash.update(chunk)\n",
    "    return filehash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c861005089beb7f09e26a5b7afa09843a0ac1ca98fe9c36ac0510a58b21da40d'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_file('data/Divvy_Stations_2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'880ba1ef2e38e4c35df4b2cd745529797f08fb24048dea0600e8174518a99869'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_file('data/Divvy_Stations_2013_corrupt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Inside a new script, `record_file_hash.py`, write code that records the hash of a CSV file inside a database, say, `tinydb`, or a CSV file. The steps I think you might want to follow are outlined below:\n",
    "\n",
    "- Create a CSV file from a `pandas.DataFrame()` (or create a [`tinydb`](http://tinydb.readthedocs.io/en/latest/) database) to store the MD5 hash of the `Divvy_Stations_2013.csv` file. Place the database (or CSV file) in the directory called `data_integrity/`. Be sure to record, at the minimum, the following:\n",
    "    - File name.\n",
    "    - Hash.\n",
    "    - Date and time on which hash was computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "- Write a test that checks that the current file hash is the value that was most recently recorded.\n",
    "- If you used a `tinydb` database, then check the API docs [here][tinydb] for more information on how to query for a particular record.\n",
    "\n",
    "[tinydb]: http://tinydb.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
